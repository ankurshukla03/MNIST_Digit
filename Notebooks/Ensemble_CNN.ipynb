{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "#import os\n",
    "#print(os.listdir(\"../input\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "b18c794b69073d1c7a36de2a16bc18c4f79a70a7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set() # setting seaborn default for plots\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.utils import np_utils\n",
    "from keras.datasets import mnist\n",
    "\n",
    "# for Convolutional Neural Network (CNN) model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.utils import np_utils\n",
    "from keras.layers import Conv2D, MaxPooling2D, ZeroPadding2D, GlobalAveragePooling2D\n",
    "from keras.layers.advanced_activations import LeakyReLU \n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "\n",
    "from keras import backend as K\n",
    "K.set_image_dim_ordering('th')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f1d9e6c7930116d4f3d05d4701893f5ce217adce",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('../input/train.csv')\n",
    "print (train.shape)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b9359e853475c32481b44505ccdd2a60b10fbe07",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv('../input/test.csv')\n",
    "print(test.shape)\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "04eb05afa40df8434cce01c339f7902cd48a014f",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Separating the labels from training dataset and making it as x_label\n",
    "y_train = train['label']\n",
    "x_train = train.drop(labels=['label'],axis=1)\n",
    "x_test = test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c996e81ae4ecc7e441e3793cec9e3b3b954899e2",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set values of the Data\n",
    "x_train = x_train.values.astype('float32') # pixel values of all images in train set\n",
    "y_train = y_train.values.astype('int32') # labels of all images\n",
    "x_test = test.values.astype('float32') # pixel values of all images in test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "277829a7e6c2bdf2c5b97a2c47d3fa14f2e9bf4d",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fix random seed for reproducibility\n",
    "random_seed = 7\n",
    "np.random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8a5e28676897433bbfbf1295340e5981cf47130a"
   },
   "source": [
    "**\n",
    "Converting Output into one hot code**\n",
    "\n",
    "A one hot encoding is a representation of categorical variables as binary vectors. This first requires that the categorical values be mapped to integer values. Then, each integer value is represented as a binary vector that is all zero values except the index of the integer, which is marked with a 1 and as this is a multi classification problem so we can convert the output class values into one-hot format which is simply a binary matrix, i.e.\n",
    "\n",
    "value 0 will be converted to one-hot format as [1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "\n",
    "value 1 will be converted to one-hot format as [0, 1, 0, 0, 0, 0, 0, 0, 0] etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e49b0b3e5cd42ae31022de8dd1b8274901f08246",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# one hot encode outputs'\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "num_classes = y_train.shape[1]\n",
    "num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "bceaf8140ee1d06c839e01e1686ef316802da51c",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reshaping the Image for CNN 2-dimesional input in [samples][pixels][width][height]\n",
    "x_train = x_train.reshape(x_train.shape[0], 1, 28, 28).astype('float32')\n",
    "x_test = x_test.reshape(x_test.shape[0], 1, 28, 28).astype('float32')\n",
    "num_pixels = x_train.shape[1]\n",
    "print (num_pixels, x_train.shape, x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b57892a84c00763f4fbeafef84b60cd8a17abcc5"
   },
   "source": [
    "# Ensemble\n",
    "Using ensemble of cnn for training and prediction. Using 10 CNNs.\n",
    "Model idea and code from [here](https://www.kaggle.com/cdeotte/25-million-images-0-99757-mnist#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "410355674a6aaa607ed7474221b267a3d1693bae",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nn = 5\n",
    "model = [0]*nn\n",
    "\n",
    "for j in range(nn):\n",
    "    model[j] = Sequential()\n",
    "    model[j].add(Conv2D(filters=32, kernel_size=(3, 3), input_shape=(1, 28, 28), activation='relu',data_format='channels_first'))\n",
    "    model[j].add(BatchNormalization())\n",
    "    model[j].add(Conv2D(filters=32, kernel_size=(3, 3), activation='relu'))\n",
    "    model[j].add(BatchNormalization())\n",
    "    model[j].add(Conv2D(32, kernel_size = 5, strides=2, padding='same', activation='relu'))\n",
    "    model[j].add(BatchNormalization())\n",
    "    model[j].add(Dropout(0.4))\n",
    "    \n",
    "    model[j].add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu'))\n",
    "    model[j].add(BatchNormalization())\n",
    "    model[j].add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu'))\n",
    "    model[j].add(BatchNormalization())\n",
    "    model[j].add(Conv2D(64, kernel_size = 5, strides=2, padding='same', activation='relu'))\n",
    "    model[j].add(BatchNormalization())\n",
    "    model[j].add(Dropout(0.4))\n",
    "    \n",
    "    model[j].add(Conv2D(128, kernel_size = 4, activation='relu'))\n",
    "    model[j].add(BatchNormalization())\n",
    "    model[j].add(Flatten())\n",
    "    model[j].add(Dropout(0.4))\n",
    "    model[j].add(Dense(10, activation='softmax'))\n",
    "    \n",
    "    # Compile model\n",
    "    model[j].compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "397289632b67068409056580832c063282b5a39d",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# With data augmentation to prevent overfitting\n",
    "datagen = ImageDataGenerator(\n",
    "        rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        zoom_range = 0.1, # Randomly zoom image \n",
    "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "        vertical_flip=False)  # randomly flip images\n",
    "\n",
    "\n",
    "datagen.fit(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f75f0f1fc6d87ab510faf578c9a9286ac27e1fcf"
   },
   "source": [
    "## Train 5 CNNS\n",
    "Everytime before the training, it divides the dataset into training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6bc642bbe349d040e3b611ccd875fa006225aa4e",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# DECREASE LEARNING RATE EACH EPOCH\n",
    "annealer = LearningRateScheduler(lambda x: 1e-3 * 0.95 ** x)\n",
    "history = [0] * nn\n",
    "epochs = 50\n",
    "\n",
    "# Fit the model\n",
    "for j in range(nn):\n",
    "    x_train2, x_val, y_train2, y_val = train_test_split(x_train, y_train, test_size = 0.10, random_state=random_seed)\n",
    "    history[j] = model[j].fit_generator(datagen.flow(x_train2,y_train2, batch_size=64),\n",
    "                              epochs = epochs, validation_data = (x_val,y_val),\n",
    "                              verbose = 0, steps_per_epoch=(len(x_train)//64),validation_steps=(len(x_val)//64),callbacks=[annealer])\n",
    "    print(\"CNN {0:d}: Epochs={1:d}, Train accuracy={2:.5f}, Validation accuracy={3:.5f}\".format(\n",
    "        j+1,epochs,max(history[j].history['acc']),max(history[j].history['val_acc']) ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ce47140d93bd39a49107042c34495a4339771b91"
   },
   "source": [
    "## Ensemble Predictions and submitting the result\n",
    "Till now I got 0.997 accuracy with just 5 models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true,
    "_uuid": "5fe728af8fc75888f4712dd8371ccb6b142cd9ed",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results = np.zeros( (x_test.shape[0],10) )\n",
    "for j in range(nn):\n",
    "    results = results + model[j].predict(x_test)\n",
    "results = np.argmax(results,axis = 1)\n",
    "results = pd.Series(results,name=\"Label\")\n",
    "submission = pd.concat([pd.Series(range(1,28001),name = \"ImageId\"),results],axis = 1)\n",
    "submission.to_csv(\"ENSEMBLE.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5f0dd7dfa2547817c5ea8b600bd003e83f717801",
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
